{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1iSFDpRBKEWr2HLlz243rbym3J2X95kcy","timestamp":1682895309397}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["A notebook for finetuning Stable Diffusion using LORA.\n","\n","Tested with [Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n","\n","Notebook developed by [pedrogengo](https://github.com/pedrogengo)."],"metadata":{"id":"EZH8PmZoncOj"}},{"cell_type":"markdown","source":["# SETUP"],"metadata":{"id":"-n_lr2oaTB58"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RKq5g6Inaq1g","executionInfo":{"status":"ok","timestamp":1682897434253,"user_tz":240,"elapsed":9943,"user":{"displayName":"Alaqian Zafar","userId":"09077679626904495344"}},"outputId":"e9840fb0-0627-4df2-c410-4ec99dd42986"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'lora' already exists and is not an empty directory.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.18.0)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.38.1)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.0+cu118)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate) (1.11.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate) (4.5.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate) (2.0.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate) (3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate) (3.12.0)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (16.0.2)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (3.25.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\n"]}],"source":["!git clone https://github.com/cloneofsimo/lora.git && sed -i 's/functools.cache/functools.lru_cache(maxsize=None)/g' /content/lora/lora_diffusion/xformers_utils.py && pip install /content/lora\n","!pip install accelerate bitsandbytes"]},{"cell_type":"markdown","source":["# TRAINING"],"metadata":{"id":"d9gKhQjMTEQ4"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lSiav5E_Z9Te","executionInfo":{"status":"ok","timestamp":1682897435306,"user_tz":240,"elapsed":1056,"user":{"displayName":"Alaqian Zafar","userId":"09077679626904495344"}},"outputId":"fddceb23-6e68-4e80-b51f-6393f7a21fed"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","import shutil\n","from google.colab import files\n","from tqdm import tqdm\n","\n","PRETRAINED_MODEL=\"runwayml/stable-diffusion-v1-5\" #@param{type: 'string'}\n","PROMPT=\"Chest X-ray\" #@param{type: 'string'}\n","\n","OUTPUT_DIR=\"Output\" #@param{type: 'string'}\n","IMAGES_FOLDER_OPTIONAL=\"/content/drive/MyDrive/Chest-X-ray-Generator/Images\" #@param{type: 'string'}\n","\n","RESOLUTION=\"512\" #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"]\n","RESOLUTION=int(RESOLUTION)\n","\n","if PRETRAINED_MODEL == \"\":\n","  print('\u001b[1;31mYou should define the pretrained model.')\n","\n","else:\n","  if IMAGES_FOLDER_OPTIONAL==\"\":\n","    INSTANCE_DIR = \"/content/data_example\"\n","    if not os.path.exists(str(INSTANCE_DIR)):\n","      %mkdir -p \"$INSTANCE_DIR\"\n","    uploaded = files.upload()\n","    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n","        shutil.move(filename, INSTANCE_DIR)\n","  else:\n","    INSTANCE_DIR = IMAGES_FOLDER_OPTIONAL\n","  \n","  if OUTPUT_DIR == \"\":\n","    OUTPUT_DIR = \"/content/output\"\n","  if not os.path.exists(str(OUTPUT_DIR)):\n","    %mkdir -p \"$OUTPUT_DIR\""],"metadata":{"id":"RXhqKsN8cEop","executionInfo":{"status":"ok","timestamp":1682897435306,"user_tz":240,"elapsed":3,"user":{"displayName":"Alaqian Zafar","userId":"09077679626904495344"}},"cellView":"form"},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["STEPS = 500 #@param {type:\"slider\", min:0, max:10000, step:10}\n","BATCH_SIZE = 1 #@param {type:\"slider\", min:0, max:128, step:1}\n","FP_16 = True #@param {type:\"boolean\"}\n","\n","#@markdown ----\n","#@markdown UNET PARAMS\n","LEARNING_RATE = 3e-4 #@param {type:\"number\"}\n","\n","#@markdown ----\n","TRAIN_TEXT_ENCODER = True #@param {type:\"boolean\"}\n","#@markdown TEXT ENCODER PARAMS\n","LEARNING_RATE_TEXT_ENCODER = 1e-6 #@param {type:\"number\"}\n","\n","NEW_LEARNING_RATE = LEARNING_RATE / BATCH_SIZE\n","NEW_LEARNING_RATE_TEXT_ENCODER = LEARNING_RATE_TEXT_ENCODER / BATCH_SIZE\n","\n","if FP_16:\n","  fp_16_arg = \"fp16\"\n","else:\n","  fp_16_arg = \"no\"\n","\n","if TRAIN_TEXT_ENCODER:\n","  command = (f'accelerate launch lora/training_scripts/train_lora_dreambooth.py '\n","             f'--pretrained_model_name_or_path=\"{PRETRAINED_MODEL}\" '\n","             f'--instance_data_dir=\"{INSTANCE_DIR}\" '\n","             f'--output_dir=\"{OUTPUT_DIR}\" '\n","             f'--instance_prompt=\"{PROMPT}\" '\n","             f'--resolution=512 '\n","             f'--use_8bit_adam '\n","             f'--mixed_precision=\"{fp_16_arg}\" '\n","             f'--train_batch_size=1 '\n","             f'--gradient_accumulation_steps=1 '\n","             f'--learning_rate={NEW_LEARNING_RATE} '\n","             f'--lr_scheduler=\"constant\" '\n","             f'--lr_warmup_steps=0 '\n","             f'--max_train_steps={STEPS} '\n","             f'--train_text_encoder '\n","             f'--lora_rank=16 '\n","             f'--learning_rate_text={NEW_LEARNING_RATE_TEXT_ENCODER}')\n","else:\n","  command = (f'accelerate launch lora/training_scripts/train_lora_dreambooth.py '\n","             f'--pretrained_model_name_or_path=\"{PRETRAINED_MODEL}\" '\n","             f'--instance_data_dir=\"{INSTANCE_DIR}\" '\n","             f'--output_dir=\"{OUTPUT_DIR}\" '\n","             f'--instance_prompt=\"{PROMPT}\" '\n","             f'--resolution=512 '\n","             f'--use_8bit_adam '\n","             f'--mixed_precision=\"{fp_16_arg}\" '\n","             f'--train_batch_size=1 '\n","             f'--gradient_accumulation_steps=1 '\n","             f'--learning_rate={NEW_LEARNING_RATE} '\n","             f'--lr_scheduler=\"constant\" '\n","             f'--lr_warmup_steps=0 '\n","             f'--lora_rank=16 '\n","             f'--max_train_steps={STEPS} '\n","             f'--learning_rate_text={NEW_LEARNING_RATE_TEXT_ENCODER}')\n","!rm -rf $INSTANCE_DIR/.ipynb_checkpoints\n","!{command}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDMXQnKfat1-","outputId":"7cc4b472-6fa7-4999-a916-29bc5d403a09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-04-30 23:30:46.906789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2;36m[23:30:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m The following values were not passed to        \u001b]8;id=169218;file:///usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\u001b\\\u001b[2mlaunch.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=41644;file:///usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py#895\u001b\\\u001b[2m895\u001b[0m\u001b]8;;\u001b\\\n","\u001b[2;36m           \u001b[0m         `accelerate launch` and had defaults used      \u001b[2m             \u001b[0m\n","\u001b[2;36m           \u001b[0m         instead:                                       \u001b[2m             \u001b[0m\n","\u001b[2;36m           \u001b[0m                 `--num_processes` was set to a value   \u001b[2m             \u001b[0m\n","\u001b[2;36m           \u001b[0m         of `\u001b[1;36m1\u001b[0m`                                         \u001b[2m             \u001b[0m\n","\u001b[2;36m           \u001b[0m                 `--num_machines` was set to a value of \u001b[2m             \u001b[0m\n","\u001b[2;36m           \u001b[0m         `\u001b[1;36m1\u001b[0m`                                            \u001b[2m             \u001b[0m\n","\u001b[2;36m           \u001b[0m                 `--mixed_precision` was set to a value \u001b[2m             \u001b[0m\n","\u001b[2;36m           \u001b[0m         of `\u001b[32m'no'\u001b[0m`                                      \u001b[2m             \u001b[0m\n","\u001b[2;36m           \u001b[0m                 `--dynamo_backend` was set to a value  \u001b[2m             \u001b[0m\n","\u001b[2;36m           \u001b[0m         of `\u001b[32m'no'\u001b[0m`                                      \u001b[2m             \u001b[0m\n","\u001b[2;36m           \u001b[0m         To avoid this warning pass in values for each  \u001b[2m             \u001b[0m\n","\u001b[2;36m           \u001b[0m         of the problematic parameters or run           \u001b[2m             \u001b[0m\n","\u001b[2;36m           \u001b[0m         `accelerate config`.                           \u001b[2m             \u001b[0m\n","2023-04-30 23:30:54.642431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of ðŸ¤— Accelerate. Use `project_dir` instead.\n","  warnings.warn(\n","Before training: Unet First Layer lora up tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","Before training: Unet First Layer lora down tensor([[-0.0124, -0.0936, -0.0147,  ..., -0.0057, -0.1163,  0.0186],\n","        [ 0.0382,  0.0300, -0.1171,  ...,  0.0834,  0.0442,  0.0429],\n","        [-0.0583, -0.0120, -0.0102,  ..., -0.0147,  0.1578, -0.0003],\n","        ...,\n","        [ 0.0389,  0.1197, -0.0802,  ...,  0.0049, -0.0639, -0.0423],\n","        [ 0.0342,  0.0359, -0.0102,  ...,  0.0485, -0.0568, -0.0247],\n","        [-0.0078, -0.0117, -0.0217,  ...,  0.0031, -0.0372, -0.0897]])\n","Before training: text encoder First Layer lora up tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","Before training: text encoder First Layer lora down tensor([[-0.0473, -0.0117, -0.1241,  ..., -0.1052,  0.0674, -0.0099],\n","        [ 0.1000, -0.0055,  0.0450,  ...,  0.0042, -0.0298, -0.0302],\n","        [-0.0333,  0.0361, -0.0840,  ...,  0.0760,  0.0322,  0.0268],\n","        ...,\n","        [-0.0612, -0.0247, -0.0117,  ..., -0.0379, -0.0118, -0.0764],\n","        [-0.0311,  0.0252, -0.0435,  ...,  0.0930,  0.0569, -0.0107],\n","        [-0.0429, -0.1031,  0.1167,  ...,  0.1181, -0.0065, -0.0420]])\n","\n","===================================BUG REPORT===================================\n","Welcome to bitsandbytes. For bug reports, please run\n","\n","python -m bitsandbytes\n","\n"," and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n","================================================================================\n","bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8013'), PosixPath('//172.28.0.1'), PosixPath('http')}\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-1bz95xdjdet41 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n","  warn(msg)\n","CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n","Either way, this might cause trouble in the future:\n","If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n","  warn(msg)\n","CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n","CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n","CUDA SETUP: Detected CUDA version 118\n","CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n","/usr/local/lib/python3.10/dist-packages/diffusers/configuration_utils.py:215: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n","  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n","***** Running training *****\n","  Num examples = 7\n","  Num batches each epoch = 7\n","  Num Epochs = 72\n","  Instantaneous batch size per device = 1\n","  Total train batch size (w. parallel, distributed & accumulation) = 1\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 500\n","Steps:  13% 63/500 [01:04<05:24,  1.35it/s, loss=0.0217, lr=0.0003]"]}]},{"cell_type":"markdown","source":["# INFERENCE"],"metadata":{"id":"-DkyZBDBTPKs"}},{"cell_type":"code","source":["#@title LOADING MODEL AND MONKEY PATCHING IT\n","import torch\n","from lora_diffusion import monkeypatch_or_replace_lora, tune_lora_scale\n","from diffusers import StableDiffusionPipeline\n","\n","\n","pipe = StableDiffusionPipeline.from_pretrained(PRETRAINED_MODEL, torch_dtype=torch.float16).to(\"cuda\")\n","monkeypatch_or_replace_lora(pipe.unet, torch.load(os.path.join(OUTPUT_DIR, \"lora_weight.pt\")))\n","monkeypatch_or_replace_lora(pipe.text_encoder, torch.load(os.path.join(OUTPUT_DIR, \"lora_weight.text_encoder.pt\")), target_replace_module=[\"CLIPAttention\"])"],"metadata":{"id":"IDArwYibSaB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe.safety_checker = None"],"metadata":{"id":"CLV5oR3BBW69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INFERENCE_PROMPT = 'heart' #@param {type:\"string\"}\n","LORA_SCALE_UNET = 0.1 #@param {type:\"number\"}\n","LORA_SCALE_TEXT_ENCODER = 0.1 #@param {type:\"number\"}\n","GUIDANCE = 1.4 #@param {type:\"slider\", min:0, max:15, step:0.2}\n","tune_lora_scale(pipe.unet, LORA_SCALE_UNET)\n","if TRAIN_TEXT_ENCODER:\n","  tune_lora_scale(pipe.text_encoder, LORA_SCALE_TEXT_ENCODER)\n","image = pipe(INFERENCE_PROMPT, num_inference_steps=50, guidance_scale=GUIDANCE).images[0]\n","image"],"metadata":{"id":"uy27Q47sa-ua"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_Gfg7tbW58_Q"},"execution_count":null,"outputs":[]}]}